# config_attention_ratio.yaml
# -------------------------------------------------------------------
# Key/Value 어텐션 비율(alpha/beta) 실험을 위한 설정 파일
# 기존 `cross_attention_finetune.py` 스크립트와 완벽하게 호환됩니다.
# -------------------------------------------------------------------

# -------------------------------------------------------------------
# 기본 학습 파라미터 (기존과 동일)
# -------------------------------------------------------------------
batch_size: 256
epochs: 100
eval_every_n_epochs: 1
fine_tune_from: pretrained_gin/checkpoints
log_every_n_steps: 50
fp16_precision: False
weight_decay: 0.000001
init_lr: 0.0005
init_base_lr: 0.0001

# -------------------------------------------------------------------
# 태스크 및 모델 설정 (기존과 동일)
# -------------------------------------------------------------------
gpu: 0
task_name: BACE
model_type: gin
model: 
  num_layer: 5
  emb_dim: 300
  feat_dim: 512
  drop_ratio: 0.3
  pool: mean

# -------------------------------------------------------------------
# 데이터셋 설정 (기존과 동일)
# -------------------------------------------------------------------
dataset:
  num_workers: 8
  valid_size: 0.1
  test_size: 0.1
  splitting: scaffold

# -------------------------------------------------------------------
# 교차-어텐션 모델을 위한 새로운 설정
# -------------------------------------------------------------------
cross_attention_specific:
  # 1. 언어 모델 설정 (기존과 동일)
  chemberta_model_name: "./ChemBERTa-77M-MLM"
  chemberta_lr: 1e-5

  # 2. 융합 모듈(CrossAttentionFusion) 설정
  fusion:
    hidden_dim: 128
    output_dim: 512
    #  새로 추가된 실험 파라미터 
    alpha: 2.0    # Key(K) 스케일링 상수. 어텐션 분포의 집중도를 조절 (1.0이 기본값)
    beta: 1.6     # Value(V) 스케일링 상수. 전달되는 정보의 총량을 조절 (1.0이 기본값)

  # 3. 최종 분류기(MLP Head) 설정 (기존과 동일)
  classifier:
    hidden_dim: 256
    dropout: 0.5